# -*- coding: utf-8 -*-
"""PatentParsingSystem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11QMCyFa2GLqLgv2GrA-ThBKT5TfFTrZN

### Problem Statement:

Consider you are a techie helping a lawyer fight a patent case, and you are assisting them by automating the finding of key phrases (keywords) across multiple patents. Your task is to create a system that can index these keywords across the documents. Each **Patent** is a text document composed of multiple lines. The goal is to build an extensible system that supports adding new keywords in the future.

### Example:

Given the patent **"AI-Assisted Diagnostics"** with the following lines:

- "AI is transforming diagnostics."
- "Deep learning is a key enabler in AI."
- "Medical diagnostics benefit from AI accuracy."

If the keywords are `["AI", "diagnostics", "learning"]`, the system should return an index such as:

- **AI**: 3 occurrences
- **diagnostics**: 2 occurrences
- **learning**: 1 occurrence

Given the patent **"Renewable Energy Storage"** with the following lines:

- "Energy storage is crucial for solar."
- "Battery and capacitor innovations help energy storage."
- "Renewable energy adoption is growing."

If the keywords are `["energy", "storage", "battery"]`, the system should return:

- **energy**: 3 occurrences
- **storage**: 2 occurrences
- **battery**: 1 occurrence

### Expectations:

1. **Index the frequency of keywords per patent.**
2. **Allow dynamic addition of new keywords without reprocessing old ones.**
3. **Provide a system that can scale for additional patents and keywords in the future.**

Can you walk me through how you would design and implement this system, and what steps you'd take to ensure it is efficient and extensible?

## Suggested Class Structure

### Patent

- `name: str`
- `keywords: Dict[str, int]`
- `lines: List[str]`

#### Methods:
- `set_keywords(keyword_list)`
- `add_keyword(keyword)`
- `get_keyword_counts()`

### PatentCollection
- Manages a collection of patents.
- Can return the overall keyword index across all patents.

---

## Goal

The goal is to build an extensible system that:

- Allows adding a collection of patents to a **Patent Collection**.
- Supports indexing a list of search keywords per patent.
- For each keyword, returns how many times it appears in the patent.
- Allows dynamically adding new keywords without reprocessing previously indexed ones.
- Should support accurate and efficient keyword matching (case-insensitive, exact match, not substring-based).
"""

def get_lines(patent):
    if patent.name == "AI-Assisted Diagnostics":
        return [
            "AI is transforming diagnostics",
            "Deep learning is a key enabler in AI",
            "Medical diagnostics benefit from AI accuracy"
        ]
    elif patent.name == "Renewable Energy Storage":
        return [
            "Energy storage is crucial for solar",
            "Battery and capacitor innovations help energy storage",
            "Renewable energy adoption is growing"
        ]

    elif patent.name=="ML and AI in 2030":
      return [
          "AI is transforming daily lives",
          "ML is the part and parcel of lives",
          "AI and ML have become a bane or boon"
      ]
    else:
        return []

import re

class Patent:

    def __init__(self, name):
        self.name = name
        self.keywords = {}
        self.lines = get_lines(self)

    def set_keywords(self, keywords):
        for keyword in keywords:
            self.add_keyword(keyword)

    def add_keyword(self, keyword):
        if keyword not in self.keywords:
            self.keywords[keyword] = self._count_keyword_in_lines(keyword)

    def _count_keyword_in_lines(self, kw):
        count = 0
        for line in self.lines:
            matches = re.findall(r'\b' + re.escape(kw) + r'\b', line, flags=re.I)
            count += len(matches)
        return count

    def get_keyword_counts(self):
        return self.keywords

class PatentCollection:
    def __init__(self, name):
        self.name = name
        self.patents = []

    def add_patent(self, patent):
        self.patents.append(patent)

    def get_keywords_index(self):
        index = {}
        for patent in self.patents:
            for keyword, count in patent.get_keyword_counts().items():
                index[keyword] = index.get(keyword, 0) + count
        return index

patent1 = Patent("AI-Assisted Diagnostics")
patent1.set_keywords(["AI", "diagnostics"])

patent2 = Patent("Renewable Energy Storage")
patent2.set_keywords(["energy", "storage"])

collection = PatentCollection("TechPatents")
collection.add_patent(patent1)

collection2= PatentCollection("EnergyPatents")
collection2.add_patent(patent2)

print(patent1.get_keyword_counts())      # {'AI': 3, 'diagnostics': 2, }
print(patent2.get_keyword_counts())      # {'energy': 3, 'storage': 2, '}


patent1.add_keyword("learning")
patent2.add_keyword("battery")

print(patent1.get_keyword_counts())      # {'AI': 3, 'diagnostics': 2, 'learning': 2}
print(patent2.get_keyword_counts())      # {'energy': 3, 'storage': 2, 'battery': 2}

patent3 = Patent("ML and AI in 2030")
collection.add_patent(patent3)
patent3.set_keywords(["ML", "AI"])
patent3.add_keyword("bane")

print(collection.name,collection.get_keywords_index())   # Updated index
print(collection2.name,collection2.get_keywords_index())  # Updated index

"""## Potential Improvement:

### Edge Case Handling:

You might want to handle empty lines or keywords like "AI!" in the future using more sophisticated NLP tools (e.g., stemming or tokenization), especially if you're scaling this beyond exact matches.
"""

